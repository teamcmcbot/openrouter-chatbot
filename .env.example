BRAND_NAME=OpenRouter Chat

# Added by [GEMINI]
# Description: OpenRouter API key for chat completions
OPENROUTER_API_KEY=your_key_here

# SUPABASE
NEXT_PUBLIC_SUPABASE_URL=your-project-url-here
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key-here
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key-here

# INTERNAL JOB SECRETS (for /api/internal/sync-models)
# Choose either Bearer token or HMAC (or both). For local dev you can use simple values.
INTERNAL_SYNC_TOKEN=dev_token
INTERNAL_SYNC_SECRET=dev_secret

# INTERNAL JOB SECRETS (for /api/internal/attachments/cleanup)
# Separate secrets for attachments cleanup internal job
INTERNAL_CLEANUP_TOKEN=dev_cleanup_token
INTERNAL_CLEANUP_SECRET=dev_cleanup_secret

# INTERNAL JOB (attachments retention)
# Retention uses the SAME secrets as cleanup via the internal cleanup auth middleware.
# No extra secrets required beyond INTERNAL_CLEANUP_TOKEN/INTERNAL_CLEANUP_SECRET above.

# INTERNAL JOB BASE URL (used by local/internal test scripts)
# Default local dev Next.js URL; override in CI or staging as needed
BASE_URL=http://localhost:3000

# INTERNAL RETENTION JOB OVERRIDES (optional; used by scripts/test-internal-retention.js)
# You can override per-tier retention thresholds. Server defaults are:
#   free=30 days, pro=60 days, enterprise=90 days
# Uncomment and adjust if you need non-default thresholds for a run.
# FREE_DAYS=30
# PRO_DAYS=60
# ENTERPRISE_DAYS=90

# Limit and dry-run flags for retention test runs
# LIMIT=1000
# DRY_RUN=1

# To use HMAC instead of Bearer in local test scripts, set:
# USE_HMAC=1

# VERCEL CRON (wrappers) SECURITY
# The GET wrappers verify this header: Authorization: Bearer ${CRON_SECRET}
CRON_SECRET=replace_me_with_strong_secret

# Optional wrapper defaults
# CRON_CLEANUP_HOURS=24
# CRON_CLEANUP_LIMIT=500
# CRON_CLEANUP_DRYRUN=false
# CRON_RETENTION_FREE_DAYS=30
# CRON_RETENTION_PRO_DAYS=60
# CRON_RETENTION_ENTERPRISE_DAYS=90
# CRON_RETENTION_LIMIT=1000
# CRON_RETENTION_DRYRUN=false

# ROOT SYSTEM PROMPT
OPENROUTER_ROOT_PROMPT_FILE=lib/prompts/root-system-prompt.txt

# Added by [GEMINI]
# Description: OpenRouter model to use
OPENROUTER_API_MODEL=deepseek/deepseek-r1-0528:free
OPENROUTER_MODELS_LIST=x-ai/grok-3-mini,anthropic/claude-3-haiku,openai/gpt-4o-mini,google/gemini-2.0-flash-exp:free,google/gemini-2.5-flash,google/gemma-3-27b-it:free,deepseek/deepseek-r1-0528:free,deepseek/deepseek-r1-0528-qwen3-8b:free,mistralai/mistral-small-3.2-24b-instruct:free,moonshotai/kimi-k2:free,qwen/qwen3-235b-a22b-07-25:free,qwen/qwen3-coder,google/gemini-2.5-flash-lite

# Added by [GEMINI]
# Description: OpenRouter base URL
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Added by [GEMINI]
# Description: Maximum tokens per response
OPENROUTER_MAX_TOKENS=1000

# Added by Phase 1: Token Management Foundation
# Description: Context allocation strategy for context-aware chat
CONTEXT_MESSAGE_PAIRS=5
CONTEXT_RATIO=0.6
OUTPUT_RATIO=0.4
RESERVE_TOKENS=150

# Phase 3: Public environment variable for frontend
NEXT_PUBLIC_ENABLE_CONTEXT_AWARE=true
NEXT_PUBLIC_ENABLE_ENHANCED_MODELS=true