BRAND_NAME=OpenRouter Chat

# Added by [GEMINI]
# Description: OpenRouter API key for chat completions
OPENROUTER_API_KEY=your_key_here

# UPSTASH REDIS
UPSTASH_REDIS_REST_URL=https://your-database-id.upstash.io
UPSTASH_REDIS_REST_TOKEN=your-token-here

# SUPABASE
NEXT_PUBLIC_SUPABASE_URL=your-project-url-here
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key-here
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key-here

# Anonymous analytics HMAC secret (server-side anon_hash derivation)
# Required for /api/chat/anonymous endpoints; keep secret and rotate if needed.
ANON_USAGE_HMAC_SECRET=replace_with_strong_secret

# INTERNAL JOB SECRETS (for /api/internal/sync-models)
# Choose either Bearer token or HMAC (or both). For local dev you can use simple values.
INTERNAL_SYNC_TOKEN=dev_token
INTERNAL_SYNC_SECRET=dev_secret

# INTERNAL JOB SECRETS (for /api/internal/attachments/cleanup)
# Separate secrets for attachments cleanup internal job
INTERNAL_CLEANUP_TOKEN=dev_cleanup_token
INTERNAL_CLEANUP_SECRET=dev_cleanup_secret

# INTERNAL JOB (attachments retention)
# Retention uses the SAME secrets as cleanup via the internal cleanup auth middleware.
# No extra secrets required beyond INTERNAL_CLEANUP_TOKEN/INTERNAL_CLEANUP_SECRET above.

# INTERNAL JOB BASE URL (used by local/internal test scripts)
# Default local dev Next.js URL; override in CI or staging as needed
BASE_URL=http://localhost:3000

# INTERNAL RETENTION JOB OVERRIDES (optional; used by scripts/test-internal-retention.js)
# You can override per-tier retention thresholds. Server defaults are:
#   free=30 days, pro=60 days, enterprise=90 days
# Uncomment and adjust if you need non-default thresholds for a run.
# FREE_DAYS=30
# PRO_DAYS=60
# ENTERPRISE_DAYS=90

# Limit and dry-run flags for retention test runs
# LIMIT=1000
# DRY_RUN=1

# To use HMAC instead of Bearer in local test scripts, set:
# USE_HMAC=1

# VERCEL CRON (wrappers) SECURITY
# The GET wrappers verify this header: Authorization: Bearer ${CRON_SECRET}
CRON_SECRET=replace_me_with_strong_secret

# Optional wrapper defaults
# CRON_CLEANUP_HOURS=24
# CRON_CLEANUP_LIMIT=500
# CRON_CLEANUP_DRYRUN=false
# CRON_RETENTION_FREE_DAYS=30
# CRON_RETENTION_PRO_DAYS=60
# CRON_RETENTION_ENTERPRISE_DAYS=90
# CRON_RETENTION_LIMIT=1000
# CRON_RETENTION_DRYRUN=false

# ROOT SYSTEM PROMPT
OPENROUTER_ROOT_PROMPT_FILE=lib/prompts/root-system-prompt.txt
OPENROUTER_USER_TRACKING=true

# Added by [GEMINI]
# Description: OpenRouter model to use
OPENROUTER_API_MODEL=deepseek/deepseek-r1-0528:free
OPENROUTER_MODELS_LIST=x-ai/grok-3-mini,anthropic/claude-3-haiku,openai/gpt-4o-mini,google/gemini-2.0-flash-exp:free,google/gemini-2.5-flash,google/gemma-3-27b-it:free,deepseek/deepseek-r1-0528:free,deepseek/deepseek-r1-0528-qwen3-8b:free,mistralai/mistral-small-3.2-24b-instruct:free,moonshotai/kimi-k2:free,qwen/qwen3-235b-a22b-07-25:free,qwen/qwen3-coder,google/gemini-2.5-flash-lite

# Added by [GEMINI]
# Description: OpenRouter base URL
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Added by [GEMINI]
# Description: Maximum tokens per response
OPENROUTER_MAX_TOKENS=1000

# Added by Phase 1: Token Management Foundation
# Description: Context allocation strategy for context-aware chat
CONTEXT_MESSAGE_PAIRS=5
CONTEXT_RATIO=0.6
OUTPUT_RATIO=0.4
RESERVE_TOKENS=150

# Phase 3: Public environment variable for frontend
NEXT_PUBLIC_ENABLE_CONTEXT_AWARE=true
NEXT_PUBLIC_ENABLE_ENHANCED_MODELS=true

# --- Streaming Debug ---
# When set to 1, enables verbose streaming diagnostics. Recommended only for local dev.
# Backend producer (lib/utils/openrouter.ts): logs request params, chunk sizes, SSE event payload size,
# emitted markers (__ANNOTATIONS_CHUNK__/__REASONING_CHUNK__), JSON parse errors, and flush.
# API transform (src/app/api/chat/stream/route.ts): logs metadata parse issues and TTF_annotation timing when first annotations are forwarded.
# Default: 0 (off)
STREAM_DEBUG=0

# Client-side streaming debug (public)
# When set to 1, enables additional client console logs tagged with [STREAM-DEBUG].
# Can also be enabled at runtime via: localStorage.setItem('DEBUG_STREAMING','1')
NEXT_PUBLIC_DEBUG_STREAMING=0

# Rollout feature flags (Phase 6)
# When set to 0, disables forwarding of special marker lines to clients (progressive annotations/reasoning).
# Default: 1 (enabled)
STREAM_MARKERS_ENABLED=1
# When set to 0, reasoning chunks are never emitted/forwarded even if requested and tier allows.
# Default: 1 (enabled)
STREAM_REASONING_ENABLED=1

# --- Auth snapshot cache (Redis) ---
# TTL in seconds for cached auth/profile snapshot used by ban/tier checks.
# If unset, code defaults to 900 (15 minutes). Lower for faster propagation after ban/unban
# at the cost of more Redis reads; higher for fewer reads but slower propagation.
# Typical values: 300, 900, 1800
AUTH_SNAPSHOT_CACHE_TTL_SECONDS=900

# --- Message length limits ---
# UI (Next.js client bundles): number of characters allowed for a single user message in the composer.
# If unset or invalid, defaults to 20000.
NEXT_PUBLIC_MAX_MESSAGE_CHARS=20000

# Server (Next.js API routes): number of characters allowed for a single user message on the server guard.
# If unset or invalid, defaults to 20000.
MAX_MESSAGE_CHARS=20000

# SENTRY
SENTRY_DSN=https://0d4190dbf3ed573c9e5b93387569bfa0@o4509938959056896.ingest.us.sentry.io/4509959677673472
SENTRY_ENABLE_DEV=true
