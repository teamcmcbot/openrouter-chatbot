{
  "data": [
    {
      "id": "qwen/qwen3-coder:free",
      "canonical_slug": "qwen/qwen3-coder-480b-a35b-07-25",
      "hugging_face_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
      "name": "Qwen: Qwen3 Coder  (free)",
      "created": 1753230546,
      "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Qwen3",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "tools",
        "tool_choice",
        "stop",
        "frequency_penalty",
        "presence_penalty",
        "seed",
        "top_k",
        "min_p",
        "repetition_penalty",
        "logprobs",
        "logit_bias",
        "top_logprobs"
      ]
    },
    {
      "id": "qwen/qwen3-coder",
      "canonical_slug": "qwen/qwen3-coder-480b-a35b-07-25",
      "hugging_face_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
      "name": "Qwen: Qwen3 Coder ",
      "created": 1753230546,
      "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Qwen3",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000003024",
        "completion": "0.0000003024",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "tools",
        "tool_choice",
        "structured_outputs",
        "response_format",
        "seed",
        "presence_penalty",
        "stop",
        "frequency_penalty",
        "repetition_penalty",
        "top_k",
        "min_p",
        "logit_bias",
        "logprobs",
        "top_logprobs"
      ]
    },
    {
      "id": "bytedance/ui-tars-1.5-7b",
      "canonical_slug": "bytedance/ui-tars-1.5-7b",
      "hugging_face_id": "ByteDance-Seed/UI-TARS-1.5-7B",
      "name": "Bytedance: UI-TARS 7B ",
      "created": 1753205056,
      "description": "UI-TARS-1.5 is a multimodal vision-language agent optimized for GUI-based environments, including desktop interfaces, web browsers, mobile systems, and games. Built by ByteDance, it builds upon the UI-TARS framework with reinforcement learning-based reasoning, enabling robust action planning and execution across virtual interfaces.\n\nThis model achieves state-of-the-art results on a range of interactive and grounding benchmarks, including OSworld, WebVoyager, AndroidWorld, and ScreenSpot. It also demonstrates perfect task completion across diverse Poki games and outperforms prior models in Minecraft agent tasks. UI-TARS-1.5 supports thought decomposition during inference and shows strong scaling across variants, with the 1.5 version notably exceeding the performance of earlier 72B and 7B checkpoints.",
      "context_length": 128000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["image", "text"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000001",
        "completion": "0.0000002",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": 2048,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "frequency_penalty",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "seed",
        "stop",
        "top_k"
      ]
    },
    {
      "id": "google/gemini-2.5-flash-lite",
      "canonical_slug": "google/gemini-2.5-flash-lite",
      "hugging_face_id": "",
      "name": "Google: Gemini 2.5 Flash Lite",
      "created": 1753200276,
      "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
      "context_length": 1048576,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["file", "image", "text"],
        "output_modalities": ["text"],
        "tokenizer": "Gemini",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000001",
        "completion": "0.0000004",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0",
        "input_cache_read": "0.000000025",
        "input_cache_write": "0.0000001833"
      },
      "top_provider": {
        "context_length": 1048576,
        "max_completion_tokens": 65535,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "tools",
        "tool_choice",
        "reasoning",
        "include_reasoning",
        "structured_outputs",
        "response_format",
        "seed"
      ]
    },
    {
      "id": "qwen/qwen3-235b-a22b-07-25:free",
      "canonical_slug": "qwen/qwen3-235b-a22b-07-25",
      "hugging_face_id": "Qwen/Qwen3-235B-A22B-Instruct-2507",
      "name": "Qwen: Qwen3 235B A22B 2507 (free)",
      "created": 1753119555,
      "description": "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\n\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Qwen3",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "stop",
        "frequency_penalty",
        "presence_penalty",
        "seed",
        "top_k",
        "min_p",
        "repetition_penalty",
        "logprobs",
        "logit_bias",
        "top_logprobs"
      ]
    },
    {
      "id": "qwen/qwen3-235b-a22b-07-25",
      "canonical_slug": "qwen/qwen3-235b-a22b-07-25",
      "hugging_face_id": "Qwen/Qwen3-235B-A22B-Instruct-2507",
      "name": "Qwen: Qwen3 235B A22B 2507",
      "created": 1753119555,
      "description": "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\n\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Qwen3",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000012",
        "completion": "0.00000059",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 262144,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "seed",
        "tools",
        "tool_choice",
        "stop",
        "frequency_penalty",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "top_k",
        "min_p",
        "structured_outputs",
        "logit_bias",
        "logprobs",
        "top_logprobs"
      ]
    },
    {
      "id": "switchpoint/router",
      "canonical_slug": "switchpoint/router",
      "hugging_face_id": "",
      "name": "Switchpoint Router",
      "created": 1752272899,
      "description": "Switchpoint AI's router instantly analyzes your request and directs it to the optimal AI from an ever-evolving library. \n\nAs the world of LLMs advances, our router gets smarter, ensuring you always benefit from the industry's newest models without changing your workflow.\n\nThis model is configured for a simple, flat rate per response here on OpenRouter. It's powered by the full routing engine from [Switchpoint AI](https://www.switchpoint.dev).",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000085",
        "completion": "0.0000034",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "reasoning",
        "include_reasoning",
        "stop",
        "top_k",
        "seed"
      ]
    },
    {
      "id": "moonshotai/kimi-k2:free",
      "canonical_slug": "moonshotai/kimi-k2",
      "hugging_face_id": "moonshotai/Kimi-K2-Instruct",
      "name": "MoonshotAI: Kimi K2 (free)",
      "created": 1752263252,
      "description": "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.",
      "context_length": 65536,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "tools",
        "tool_choice",
        "stop",
        "frequency_penalty",
        "presence_penalty",
        "seed",
        "top_k",
        "min_p",
        "repetition_penalty",
        "logprobs",
        "logit_bias",
        "top_logprobs"
      ]
    },
    {
      "id": "moonshotai/kimi-k2",
      "canonical_slug": "moonshotai/kimi-k2",
      "hugging_face_id": "moonshotai/Kimi-K2-Instruct",
      "name": "MoonshotAI: Kimi K2",
      "created": 1752263252,
      "description": "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.",
      "context_length": 63000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000014",
        "completion": "0.00000249",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 63000,
        "max_completion_tokens": 63000,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "tools",
        "tool_choice",
        "structured_outputs",
        "response_format",
        "stop",
        "frequency_penalty",
        "presence_penalty",
        "top_k",
        "repetition_penalty",
        "logit_bias",
        "min_p",
        "logprobs",
        "top_logprobs",
        "seed"
      ]
    },
    {
      "id": "thudm/glm-4.1v-9b-thinking",
      "canonical_slug": "thudm/glm-4.1v-9b-thinking",
      "hugging_face_id": "THUDM/GLM-4.1V-9B-Thinking",
      "name": "THUDM: GLM 4.1V 9B Thinking",
      "created": 1752244385,
      "description": "GLM-4.1V-9B-Thinking is a 9B parameter vision-language model developed by THUDM, based on the GLM-4-9B foundation. It introduces a reasoning-centric \"thinking paradigm\" enhanced with reinforcement learning to improve multimodal reasoning, long-context understanding (up to 64K tokens), and complex problem solving. It achieves state-of-the-art performance among models in its class, outperforming even larger models like Qwen-2.5-VL-72B on a majority of benchmark tasks. ",
      "context_length": 65536,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["image", "text"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000000035",
        "completion": "0.000000138",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": 8000,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "reasoning",
        "include_reasoning",
        "stop",
        "frequency_penalty",
        "presence_penalty",
        "seed",
        "top_k",
        "min_p",
        "repetition_penalty",
        "logit_bias"
      ]
    },
    {
      "id": "mistralai/devstral-medium",
      "canonical_slug": "mistralai/devstral-medium-2507",
      "hugging_face_id": "",
      "name": "Mistral: Devstral Medium",
      "created": 1752161321,
      "description": "Devstral Medium is a high-performance code generation and agentic reasoning model developed jointly by Mistral AI and All Hands AI. Positioned as a step up from Devstral Small, it achieves 61.6% on SWE-Bench Verified, placing it ahead of Gemini 2.5 Pro and GPT-4.1 in code-related tasks, at a fraction of the cost. It is designed for generalization across prompt styles and tool use in code agents and frameworks.\n\nDevstral Medium is available via API only (not open-weight), and supports enterprise deployment on private infrastructure, with optional fine-tuning capabilities.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Mistral",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000004",
        "completion": "0.000002",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "tools",
        "tool_choice",
        "stop",
        "frequency_penalty",
        "presence_penalty",
        "response_format",
        "structured_outputs",
        "seed"
      ]
    },
    {
      "id": "mistralai/devstral-small",
      "canonical_slug": "mistralai/devstral-small-2507",
      "hugging_face_id": "mistralai/Devstral-Small-2507",
      "name": "Mistral: Devstral Small 1.1",
      "created": 1752160751,
      "description": "Devstral Small 1.1 is a 24B parameter open-weight language model for software engineering agents, developed by Mistral AI in collaboration with All Hands AI. Finetuned from Mistral Small 3.1 and released under the Apache 2.0 license, it features a 128k token context window and supports both Mistral-style function calling and XML output formats.\n\nDesigned for agentic coding workflows, Devstral Small 1.1 is optimized for tasks such as codebase exploration, multi-file edits, and integration into autonomous development agents like OpenHands and Cline. It achieves 53.6% on SWE-Bench Verified, surpassing all other open models on this benchmark, while remaining lightweight enough to run on a single 4090 GPU or Apple silicon machine. The model uses a Tekken tokenizer with a 131k vocabulary and is deployable via vLLM, Transformers, Ollama, LM Studio, and other OpenAI-compatible runtimes.\n",
      "context_length": 128000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Mistral",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000007",
        "completion": "0.00000028",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p",
        "tools",
        "tool_choice",
        "stop",
        "frequency_penalty",
        "presence_penalty",
        "response_format",
        "structured_outputs",
        "seed",
        "min_p",
        "repetition_penalty",
        "top_k"
      ]
    }
  ]
}
