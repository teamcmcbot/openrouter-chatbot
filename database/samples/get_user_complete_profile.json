{
  "id": "f319ca56-4197-477c-92e7-e6e2d95884be",
  "email": "txxxxxxxxt@gmail.com",
  "credits": 0,
  "full_name": "RoyalePros",
  "avatar_url": "https://lh3.googleusercontent.com/a/ACg8ocKuKrmY5-3-9sC4mv09Z8_DOxbjsIV3sY27yE9UXgM_jjumvfk=s96-c",
  "timestamps": {
    "created_at": "2025-07-17T08:23:07.148825+00:00",
    "updated_at": "2025-08-06T14:21:15.192697+00:00",
    "last_active": "2025-08-06T14:21:15.192697+00:00"
  },
  "preferences": {
    "ui": {
      "theme": "dark",
      "auto_save": true,
      "sidebar_width": 280,
      "show_token_count": true,
      "code_highlighting": true
    },
    "model": {
      "temperature": 0.7,
      "default_model": "google/gemini-2.5-flash",
      "system_prompt": "You are a serious mentor. Do not sugar-coat and be honest in your response, especially if the user might be wrong or confuse in his comments or questions."
    },
    "session": {
      "auto_title": true,
      "max_history": 10,
      "save_anonymous": false
    }
  },
  "usage_stats": {
    "today": {
      "models_used": {
        "openai/gpt-oss-20b": 1,
        "openai/gpt-oss-120b": 1,
        "openrouter/horizon-beta": 3
      },
      "input_tokens": 2703,
      "total_tokens": 7090,
      "messages_sent": 5,
      "output_tokens": 4387,
      "active_minutes": 31,
      "sessions_created": 4,
      "messages_received": 5
    },
    "all_time": {
      "last_reset": "2025-07-17 08:23:07.148825+00",
      "total_tokens": 606271,
      "total_messages": 1184,
      "sessions_created": 8
    },
    "recent_days": [
      {
        "usage_date": "2025-08-06",
        "models_used": {
          "openai/gpt-oss-20b": 1,
          "openai/gpt-oss-120b": 1,
          "openrouter/horizon-beta": 3
        },
        "total_tokens": 7090,
        "messages_sent": 5,
        "active_minutes": 31,
        "sessions_created": 4,
        "messages_received": 5
      },
      {
        "usage_date": "2025-08-05",
        "models_used": {
          "z-ai/glm-4.5-air:free": 3,
          "openrouter/horizon-beta": 2
        },
        "total_tokens": 1963,
        "messages_sent": 5,
        "active_minutes": 11,
        "sessions_created": 4,
        "messages_received": 5
      },
      {
        "usage_date": "2025-08-03",
        "models_used": { "qwen/qwen3-coder": 2 },
        "total_tokens": 377,
        "messages_sent": 2,
        "active_minutes": 11,
        "sessions_created": 0,
        "messages_received": 2
      },
      {
        "usage_date": "2025-08-02",
        "models_used": {
          "qwen/qwen3-coder": 1,
          "google/gemini-2.5-flash-lite": 13
        },
        "total_tokens": 5413,
        "messages_sent": 14,
        "active_minutes": 27,
        "sessions_created": 0,
        "messages_received": 14
      },
      {
        "usage_date": "2025-08-01",
        "models_used": {
          "moonshotai/kimi-k2:free": 4,
          "google/gemini-2.5-flash-lite": 1,
          "google/gemini-2.0-flash-exp:free": 3
        },
        "total_tokens": 10526,
        "messages_sent": 8,
        "active_minutes": 82,
        "sessions_created": 0,
        "messages_received": 8
      },
      {
        "usage_date": "2025-07-31",
        "models_used": { "google/gemini-2.5-flash-lite": 1 },
        "total_tokens": 1495,
        "messages_sent": 1,
        "active_minutes": 4,
        "sessions_created": 0,
        "messages_received": 1
      },
      {
        "usage_date": "2025-07-30",
        "models_used": {
          "qwen/qwen3-coder": 34,
          "x-ai/grok-3-mini": 24,
          "google/gemini-2.5-flash": 1,
          "moonshotai/kimi-k2:free": 38,
          "google/gemini-2.5-flash-lite": 280,
          "deepseek/deepseek-r1-0528:free": 5,
          "qwen/qwen3-235b-a22b-07-25:free": 57
        },
        "total_tokens": 492855,
        "messages_sent": 449,
        "active_minutes": 2779,
        "sessions_created": 0,
        "messages_received": 439
      }
    ]
  },
  "available_models": [
    {
      "model_id": "moonshotai/kimi-k2:free",
      "model_name": "MoonshotAI: Kimi K2 (free)",
      "model_tags": [],
      "daily_limit": null,
      "monthly_limit": null,
      "model_description": "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training."
    },
    {
      "model_id": "openai/gpt-oss-20b",
      "model_name": "OpenAI: GPT OSS 20B",
      "model_tags": [],
      "daily_limit": null,
      "monthly_limit": null,
      "model_description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs."
    },
    {
      "model_id": "google/gemini-2.5-flash",
      "model_name": "Google: Gemini 2.5 Flash",
      "model_tags": [],
      "daily_limit": null,
      "monthly_limit": null,
      "model_description": "Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."
    },
    {
      "model_id": "openai/gpt-oss-120b",
      "model_name": "OpenAI: GPT OSS 120B",
      "model_tags": [],
      "daily_limit": null,
      "monthly_limit": null,
      "model_description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation."
    },
    {
      "model_id": "anthropic/claude-opus-4.1",
      "model_name": "Anthropic: Claude Opus 4.1",
      "model_tags": [],
      "daily_limit": null,
      "monthly_limit": null,
      "model_description": "Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning."
    },
    {
      "model_id": "x-ai/grok-3-mini",
      "model_name": "xAI: Grok 3 Mini",
      "model_tags": [],
      "daily_limit": null,
      "monthly_limit": null,
      "model_description": "A lightweight model that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible."
    }
  ],
  "subscription_tier": "enterprise"
}
