# Streaming Chat Architecture

**Date**: August 23, 2025  
**Version**: 1.0  
**Status**: Production Ready  

## Overview

This document provides a comprehensive technical overview of the streaming chat implementation, including the integration with OpenRouter's streaming API, Vercel AI SDK usage, data transformation pipeline, and frontend real-time display architecture.

## Architecture Overview

```mermaid
graph TB
    A[User Types Message] --> B[MessageInput Component]
    B --> C{Streaming Enabled?}
    C -->|Yes| D[useChatStreaming Hook]
    C -->|No| E[useChat Hook]
    
    D --> F[POST /api/chat/stream]
    E --> G[POST /api/chat]
    
    F --> H[OpenRouter Streaming API]
    G --> I[OpenRouter Standard API]
    
    H --> J[Stream Processing Pipeline]
    J --> K[ReadableStream Chunks]
    K --> L[Frontend Stream Reader]
    L --> M[Progressive UI Updates]
    
    I --> N[Complete Response]
    N --> O[Single UI Update]
    
    M --> P[Database Sync]
    O --> P
    P --> Q[Message Persistence]
```

## OpenRouter Streaming Integration

### OpenRouter API Response Format

OpenRouter's streaming endpoint (`/chat/completions`) returns Server-Sent Events (SSE) with the following chunk structure:

```typescript
// Individual stream chunks
{
  "id": "gen-1755941033-rhZq1U5diW1edvt4mMdt",
  "provider": "Google",
  "model": "google/gemini-2.5-flash-lite",
  "object": "chat.completion.chunk",
  "created": 1755941033,
  "choices": [
    {
      "index": 0,
      "delta": {
        "role": "assistant",
        "content": "Let me think about this...",
        "reasoning": "**Analysis Step 1**\n\nI need to...",
        "reasoning_details": [
          {
            "type": "reasoning.text",
            "text": "Detailed reasoning chunk",
            "format": "unknown",
            "index": 0
          }
        ]
      },
      "finish_reason": null
    }
  ]
}
```

### Key Characteristics

1. **Incremental Content**: `delta.content` contains text chunks that build up the response
2. **Reasoning Data**: `delta.reasoning` and `delta.reasoning_details` arrive before content
3. **Final Metadata**: Last chunk contains `usage` statistics and completion information
4. **Structured Data**: Annotations, web search results embedded in stream

## Vercel AI SDK v5 Integration

### Why Vercel AI SDK?

The Vercel AI SDK provides critical infrastructure for handling streaming responses:

1. **Stream Parsing**: Handles SSE parsing and chunk extraction
2. **Response Creation**: `createTextStreamResponse` creates proper streaming HTTP responses
3. **Error Handling**: Built-in retry logic and error boundaries
4. **Type Safety**: TypeScript definitions for OpenRouter response formats

### SDK Implementation

```typescript
// Backend: src/app/api/chat/stream/route.ts
import { createTextStreamResponse } from 'ai';

export async function POST(request: NextRequest): Promise<Response> {
  // Authentication, rate limiting, etc.
  
  const openrouterResponse = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model,
      messages,
      stream: true,
      // ... other parameters
    }),
  });

  // Transform OpenRouter stream to our format
  return createTextStreamResponse({
    // Stream transformation logic
  });
}
```

### Why Not AI SDK React Hooks?

The current AI SDK v5 React hooks have limitations for our use case:

1. **Limited Metadata Access**: Hooks don't expose usage tokens, completion IDs
2. **Custom Processing**: We need custom reasoning data extraction
3. **Database Integration**: Requires manual sync with our message persistence
4. **Error Handling**: Custom error handling for rate limits and authentication

## Data Transformation Pipeline

### Backend Processing (`/api/chat/stream`)

```typescript
// Stream processing pipeline
const streamMetadata = {
  reasoning: '',
  reasoning_details: [] as Record<string, unknown>[],
  usage: null,
  id: null,
  annotations: [] as any[],
};

// Process each chunk
for await (const chunk of stream) {
  try {
    const data = JSON.parse(chunk.replace('data: ', ''));
    
    // Extract incremental content
    if (data.choices?.[0]?.delta?.content) {
      fullContent += data.choices[0].delta.content;
      // Send content chunk to frontend
      yield data.choices[0].delta.content;
    }
    
    // Accumulate reasoning data (comes first)
    if (data.choices?.[0]?.delta?.reasoning) {
      streamMetadata.reasoning += data.choices[0].delta.reasoning;
    }
    
    if (data.choices?.[0]?.delta?.reasoning_details) {
      streamMetadata.reasoning_details.push(...data.choices[0].delta.reasoning_details);
    }
    
    // Capture final metadata
    if (data.usage) {
      streamMetadata.usage = data.usage;
      streamMetadata.id = data.id;
    }
  } catch (error) {
    // Handle chunk parsing errors
  }
}

// Send final metadata
yield `\n\n__FINAL_METADATA__${JSON.stringify({
  __FINAL_METADATA__: {
    response: fullContent,
    usage: streamMetadata.usage,
    id: streamMetadata.id,
    reasoning: streamMetadata.reasoning,
    reasoning_details: streamMetadata.reasoning_details,
    // ... other metadata
  }
})}`;
```

### Frontend Processing (`useChatStreaming`)

```typescript
// Frontend stream consumption
const reader = response.body.getReader();
const decoder = new TextDecoder();
let buffer = '';
let finalMetadata = null;

try {
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    const chunk = decoder.decode(value, { stream: true });
    buffer += chunk;
    
    // Check for metadata marker
    if (buffer.includes('__FINAL_METADATA__')) {
      const [content, metadataJson] = buffer.split('__FINAL_METADATA__');
      
      // Update streaming content
      setStreamingContent(content);
      
      // Parse final metadata
      try {
        finalMetadata = JSON.parse(metadataJson)?.__FINAL_METADATA__;
      } catch (error) {
        console.error('Metadata parsing error:', error);
      }
      
      buffer = '';
    } else {
      // Regular content update
      setStreamingContent(buffer);
    }
  }
} finally {
  reader.releaseLock();
}
```

## Frontend Real-Time Display

### Component Architecture

```typescript
// MessageInput.tsx - Streaming Toggle
const MessageInput = () => {
  const { streamingEnabled, setStreamingEnabled } = useSettingsStore();
  
  return (
    <div className="flex items-center gap-2">
      <StreamingToggle 
        enabled={streamingEnabled}
        onToggle={setStreamingEnabled}
      />
      {/* Other controls */}
    </div>
  );
};

// useChatStreaming.ts - Streaming Hook
export const useChatStreaming = () => {
  const [streamingContent, setStreamingContent] = useState('');
  const [isStreaming, setIsStreaming] = useState(false);
  
  const sendMessage = async (message: string) => {
    setIsStreaming(true);
    
    try {
      const response = await fetch('/api/chat/stream', {
        method: 'POST',
        body: JSON.stringify({ message }),
      });
      
      // Process stream (shown above)
      
    } finally {
      setIsStreaming(false);
    }
  };
  
  return { sendMessage, streamingContent, isStreaming };
};

// MessageList.tsx - Progressive Display
const MessageList = () => {
  return (
    <div>
      {messages.map(message => (
        <div key={message.id}>
          {message.isStreaming ? (
            <StreamingMessage content={streamingContent} />
          ) : (
            <CompleteMessage message={message} />
          )}
        </div>
      ))}
    </div>
  );
};
```

### Real-Time Updates

1. **Progressive Text**: Content appears character by character as chunks arrive
2. **Animated Cursor**: Blinking cursor indicates streaming in progress
3. **Markdown Rendering**: `ReactMarkdown` handles all content (text and markdown)
4. **Reasoning Display**: Reasoning data appears before main content for better UX

### State Management

```typescript
// useSettingsStore.ts - Persistent Settings
interface SettingsStore {
  streamingEnabled: boolean;
  setStreamingEnabled: (enabled: boolean) => void;
}

// useChatStore.ts - Message State
interface ChatStore {
  conversations: Conversation[];
  addMessage: (message: ChatMessage) => void;
  updateStreamingMessage: (content: string) => void;
}
```

## Database Integration

### Message Persistence Pipeline

```typescript
// After streaming completes
const assistantMessage: ChatMessage = {
  id: `msg_${Date.now() + 1}`,
  content: finalContent,
  role: "assistant",
  timestamp: new Date(),
  user_message_id: userMessage.id,
  model,
  contentType: "markdown",
  total_tokens: finalMetadata?.usage?.total_tokens || 0,
  input_tokens: finalMetadata?.usage?.prompt_tokens || 0,
  output_tokens: finalMetadata?.usage?.completion_tokens || 0,
  elapsed_ms: finalMetadata?.elapsed_ms || 0,
  completion_id: finalMetadata?.id,
  reasoning: finalMetadata?.reasoning,
  reasoning_details: finalMetadata?.reasoning_details,
  annotations: finalMetadata?.annotations,
  has_websearch: !!finalMetadata?.has_websearch,
  websearch_result_count: finalMetadata?.websearch_result_count || 0,
};

// Sync to database via existing endpoint
await fetch('/api/chat/messages', {
  method: 'POST',
  body: JSON.stringify({
    sessionId: conversationId,
    messages: [userMessage, assistantMessage],
  }),
});
```

### Database Schema Compatibility

The streaming implementation maintains full compatibility with the existing database schema:

- **chat_messages**: All fields populated identically to non-streaming
- **chat_attachments**: Image attachments work with streaming
- **Metadata**: Reasoning, web search, token usage all preserved

## Advanced Features

### Reasoning Integration

Reasoning data flows through the stream with special handling:

```typescript
// Reasoning appears before content
if (data.choices?.[0]?.delta?.reasoning) {
  streamMetadata.reasoning += data.choices[0].delta.reasoning;
}

if (data.choices?.[0]?.delta?.reasoning_details) {
  streamMetadata.reasoning_details.push(...data.choices[0].delta.reasoning_details);
}

// UI displays reasoning before content
{message.role === "assistant" && (
  (typeof message.reasoning === 'string' && message.reasoning.trim().length > 0) ||
  (Array.isArray(message.reasoning_details) && message.reasoning_details.length > 0)
) && (
  <ReasoningSection 
    reasoning={message.reasoning}
    details={message.reasoning_details}
  />
)}
```

### Image Attachment Support

Streaming works seamlessly with image attachments:

1. **Multimodal Requests**: Images encoded as base64 in OpenRouter request
2. **Stream Processing**: No changes needed - images processed before streaming
3. **Database Linking**: Attachments linked to messages after stream completion

### Web Search Integration

Web search annotations flow through the stream:

```typescript
// Backend extracts annotations from stream
const annotations = extractAnnotations(streamMetadata);

// Frontend displays alongside content
{message.annotations && (
  <AnnotationsDisplay annotations={message.annotations} />
)}
```

## Error Handling & Edge Cases

### Stream Interruption

```typescript
// Frontend handles stream errors gracefully
try {
  // Stream processing
} catch (error) {
  console.error('Stream error:', error);
  
  // Fallback to non-streaming
  const fallbackResponse = await fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify(requestBody),
  });
  
  // Continue with regular processing
}
```

### Network Issues

- **Retry Logic**: Automatic retry for transient failures
- **Graceful Degradation**: Falls back to non-streaming mode
- **User Feedback**: Clear error messages and recovery options

### Rate Limiting

Streaming respects the same rate limiting as non-streaming:

```typescript
// Tiered rate limiting applies to streaming endpoints
export const POST = withProtectedAuth(
  withTieredRateLimit(handler, { tier: "tierA" }) // Chat endpoints
);
```

## Performance Characteristics

### Latency Improvements

- **Time to First Token**: ~200-500ms vs 2-10s for complete response
- **Perceived Speed**: Users see response immediately
- **Engagement**: Higher user satisfaction with streaming

### Resource Usage

- **Memory**: Minimal - processes chunks as they arrive
- **CPU**: Low overhead - simple string concatenation
- **Network**: Efficient - no additional requests vs non-streaming

### Scaling Considerations

- **Concurrent Streams**: Each stream maintains minimal server state
- **Connection Limits**: Standard HTTP connection pooling applies
- **Database Load**: Identical to non-streaming (single write at end)

## Testing Strategy

### Unit Tests

```typescript
// Test streaming hook
describe('useChatStreaming', () => {
  it('processes stream chunks correctly', async () => {
    // Mock streaming response
    // Verify progressive updates
  });
  
  it('handles metadata parsing', async () => {
    // Test __FINAL_METADATA__ extraction
  });
});
```

### Integration Tests

```typescript
// Test full streaming pipeline
describe('Streaming Pipeline', () => {
  it('streams complete conversation', async () => {
    // End-to-end streaming test
  });
  
  it('syncs to database correctly', async () => {
    // Verify message persistence
  });
});
```

### Performance Tests

- **Load Testing**: Multiple concurrent streams
- **Stress Testing**: Large responses, network interruptions  
- **Latency Testing**: Time to first token measurements

## Security Considerations

### Authentication

Streaming endpoints use identical authentication to non-streaming:

```typescript
export const POST = withProtectedAuth(streamingHandler);
```

### Rate Limiting

Tiered rate limiting prevents abuse:

```typescript
// Chat tier (most restrictive) - 10/20/200/500 requests/hour
export const POST = withTieredRateLimit(handler, { tier: "tierA" });
```

### Data Validation

All inputs validated identically to non-streaming endpoints.

## Monitoring & Observability

### Logging

```typescript
logger.info('Chat stream request received', {
  userId: user.id,
  model,
  streamEnabled: true,
  elapsed_ms: elapsedMs,
});
```

### Metrics

- **Stream Duration**: Time from start to completion
- **Chunk Count**: Number of chunks processed
- **Error Rate**: Failed streams vs successful
- **User Adoption**: Streaming vs non-streaming usage

## Future Enhancements

### Real-Time Reasoning Display

Show reasoning chunks as they arrive (currently planned):

```typescript
// Progressive reasoning updates
if (data.choices?.[0]?.delta?.reasoning) {
  setStreamingReasoning(prev => prev + data.choices[0].delta.reasoning);
}
```

### Stream Persistence

Cache stream state for recovery:

```typescript
// Save stream state periodically
const streamState = {
  content: currentContent,
  metadata: partialMetadata,
  timestamp: Date.now(),
};
```

### Advanced UI Features

- **Stream Speed Control**: Adjust playback speed
- **Stream Pause/Resume**: User control over streaming
- **Multi-Stream Management**: Handle multiple concurrent conversations

---

## Conclusion

The streaming implementation provides a robust, scalable solution for real-time chat responses while maintaining full feature parity with non-streaming mode. The architecture leverages proven technologies (Vercel AI SDK, OpenRouter API) with custom enhancements for reasoning data, image attachments, and web search integration.

**Key Benefits:**
- ‚ö° **Immediate Feedback**: Users see responses as they're generated
- üîÑ **Full Feature Parity**: All features work identically in streaming mode  
- üõ°Ô∏è **Production Ready**: Comprehensive error handling and security
- üìä **Observable**: Full logging and metrics for monitoring
- üß™ **Testable**: Comprehensive test coverage for reliability

The implementation successfully transforms the chat experience from "request ‚Üí wait ‚Üí response" to "request ‚Üí immediate stream ‚Üí enhanced response", significantly improving user engagement and satisfaction.
